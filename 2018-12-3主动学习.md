---
title: 2018-12-3主动学习
tags: 主动学习
grammar_abbr: true
grammar_table: true
grammar_defList: true
grammar_emoji: true
grammar_footnote: true
grammar_ins: true
grammar_mark: true
grammar_sub: true
grammar_sup: true
grammar_checkbox: true
grammar_mathjax: true
grammar_flow: true
grammar_sequence: true
grammar_plot: true
grammar_code: true
grammar_highlight: true
grammar_html: true
grammar_linkify: true
grammar_typographer: true
grammar_video: true
grammar_audio: true
grammar_attachment: true
grammar_mermaid: true
grammar_classy: true
grammar_cjkEmphasis: true
grammar_cjkRuby: true
grammar_center: true
grammar_align: true
grammar_tableExtra: true
---


## 主动学习（Active Learning）

在过去，我们需要搜集大量的数据，并且利用专家标注这些数据，标注的越多，越准确，我们的训练集越优秀，那么我们的预测准确率就会越高，这种方式叫作被动学习（Passive Learning）。缺点是需要花费大量人力物力来寻找优秀的数据。

而主动学习（Active Learning）则为了解决这个缺陷而生，它一开始只寻找少量数据，这些少量数据是样本集合中最令人头疼的数据，也是区分类别最重要的指示，将这些小数据交给专家来标注后，训练新的模型，再寻找更令人头疼的但很重要的数据，如此重复。这样的做法可以极大地节省专家标注的数据量以及我们训练的时间。

#### MNIST 数据集下的模拟主动学习与非主动学习的对比
![正常的训练 V.S. 主动学习 （在一个epoch内）](./images/1543808026878.png)

![正常的训练 V.S. 主动学习 （在一个epoch内，数据样本有偏，含有极少的数字3）](./images/1543808097412.png)
可以看出当数据有偏时，主动学习可以挑选更重要的样本来学习到更好的模型。

![正常的训练 V.S. 主动学习 （在一个epoch内，数据样本有偏，含有极少的数字3）](./images/1543808208733.png)
但是当继续训练时，数据较少的样本不再出现，则准确率会下降。

![正常的训练 V.S. 主动学习 V.S. 正常的训练重采样 V.S. 主动学习重采样](./images/1543808247531.png)
为了解决较少的问题，于是采用重采样来使得已出现的数据和新数据混合出现。


![多epoch情况下的对比](./images/1543808351419.png)
在多轮epoches的情况下，主动学习表现出明显的优势

![多epoch情况下的对比（数据样本有偏，含有极少的数字3）](./images/1543808366622.png)
也因为在多轮epoches下，重采样不占据优势。


#### 主动学习流程

![Pool-Based Sampling](./images/1543816836667.png)

 1. 获取数据（Gather data）
	 获取大量的未标注数据
 2. 分离数据（Split data into seed and unlabelled data）
	 初始我们需要一个可用的基本模型，所以我们需要先把数据分离成一部分需要标注的种子数据，以便训练一个初始模型。另一部大量的未标注数据将会被用来做主动学习。
 3. 训练模型（Train the model）
	 利用种子数据训练初始模型。
 4. 筛选未标注数据（Choose unlabelled instances）
	 我们需要利用某些方法来挑选信息量最高或是最令人迷惑的数据作为主动学习的数据，提供给专家标注，标注后加入模型训练。
 5. 设定停止指标（Stopping criteria）
	 对于第三步和第四步，我们需要反复迭代更新模型，我们可以设定一个停止指标来决定何时我们可以停止迭代更新模型。比如设定一个训练的轮次，或者利用验证集和测试集来衡量模型效果并决定何时停止训练。


#### 主动学习的方法

 1. 基于人类反馈（Human Feedback）
	 极大地改善用极少的样本构建模型的效果
 2. 半监督和迁移学习（Semi-supervised and Transfer Learning）
	 改善学习过程和增广稀少的已标注数据
	- 半监督学习
	- 迁移学习

主动学习算法的核心问题是：设计一个精准策略去查询未标注数据。

查询策略：
 - 基于异质数据的模型：数据样本来源于数据不相似程度高的区域
	 - uncertainty sampling
	For binary classifer
	```mathjax!
	$$En(\bar{X}) = \sum_{i=1}^{k} ||p_i - 0.5||$$ 
	```
	For  k classes （其实就是 Entropy 公式）
	```mathjax!
	$$En(\bar{X}) = -\sum_{i=1}^{k} p_i·log(p_i)$$ 
	```
	Gini-index 基尼系数
	```mathjax!
	$$G(\bar{X}) = 1-\sum_{i=1}^{k} p_i^2$$ 
	```
	由于有些数据的分布不平衡，有些类别数据少，有些多，所以需要做一个权衡，添加一个权重系数`!$w_i$`，这个权重由数据的占比生成，以后替换成`!$p_i·w_i$`。
	 - query-by-committtee
		多个分类器投票，对于一个样本，在不同分类器结果最不同的则是我们需要的数据。
	 - expected model change
		 在基于梯度训练模型中，我们计算一个样本的梯度和其后验概率来决定它的不确定性。
	```mathjax!
	$$C(\bar{X}) = \sum_{i=1}^{k} p_i·\delta g_i$$ 
	```
 - 基于性能的模型：直接衡量误差或者方差的减少
	 - expected error reduction
		 相对于最大化要被作为查询样本的不确定性，这个方法要最小化剩余未标注数据集的不确定性。简而言之，当一个样本被从未标注数据集拿出来后，这个未标注数据集的不确定性应该降低。
    以下公式中 `!$p_i(\bar{X})$`是样本X被加入模型之前的后验概率，而`!$P_j^{\bar{X},i}$`是X被加入后在类别j上的后验概率。
	对于二分类问题（这里需要最大化）
	```mathjax!
	$$E(\bar{X}, V) = \sum_{i=1}^{k} p_i(\bar{X})·(\sum_{j=1}^k\sum_{\bar{Z}\in V} ||P_j^{\bar{X},i}-0.5||)$$	
	```
	而对于k分类问题（也就是替换成计算Entropy，这里却是需要最小化了）
	```mathjax!
	$$E(\bar{X}, V) = \sum_{i=1}^{k} p_i(\bar{X})·(\sum_{j=1}^k\sum_{\bar{Z}\in V} -P_j^{\bar{X},i}·log(P_j^{\bar{X},i}))$$	
	```
	 - expected variance reduction
		 由于直接计算error会消耗大量计算力，而error可以表达为真实噪音误差加上偏差和方差，而我们知道当一组数据集的error降低，可以对应于方差也是降低的。而当一个样本被选择后，模型的误差很高的依赖于方差的变化。因此，我们可以只降低方差来代替误差。这样做的主要优势就是计算大大简化。
 - 基于表征的模型：试图创建一组数据能直接表征类别的核心特征
     ```mathjax!
	$$O(\bar{X}, V)=H(\bar{X})·R(\bar{X}, V)$$	
	```
	在上面式子中，H代表异质数据方法，R代表表征的方法。（其实H就是Entropy，而R就是向量或是概率分布，且这个向量要与未标注的整体集合的质心相似）
	 - density-based model
	   

总的来说，基于异质数据的方法是针对查询样本，而基于误差衡量的是针对聚合本身。至于基于表征的方法既要考虑异质数据的方法，也要有一定的代表性。还有一种混合的方法，即挑选的未标注数据：

 - 有信息量（Informative）：不确定性高
 - 有代表性（Representative）：不像噪音、能代表一组未标注的数据

    
- [ ] Cost-Effective Active Learning for Deep Image Classification (Sun Yat-sen University, 2017) 
- [ ] Adversarial Active Learning for Deep Networks:a Margin Based Approach (University of Nice Cote dAzur, 2018) 
- [ ] Deep Bayesian Active Learning with Image Data (University of Cambridge, 2017) 
- [ ] Active Learning for Convolutional Neural Networks: A Core-set Approach (Stanford University, 2017) 
- [ ] Learning Active Learning from Data (CVLab, EPFL, 2017) 
- [ ] A Meta-Learning Approach to One-Step Active-Learning (Sorbonne Universites, 2017)

